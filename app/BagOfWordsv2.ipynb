{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "987a1352-ff51-4888-93fb-8cab1445e923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\migle\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\migle\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60dfb352-726b-49e2-abb5-dd5f1c4fd4cd",
   "metadata": {},
   "source": [
    "Bag Of Words:\n",
    "A bag of words(BoW) is essentially just a list/matrix of all the words we see. No caring about order or hierarchy, it is simply a list of the amount of times each word shows up.\n",
    "\n",
    "I believe we used this in the Neural Network Assignment, as a bag of words is a way to turn the text into numbers so the computer can read it. \n",
    "An example would be: Bag Of Words listed that 9 out of 10 times that the word \"Good\" showed up, the label was \"Positive\". This must mean that Good is Positive.\n",
    "\n",
    "In our case, I think what Frederik expects us to do is to build a bag of words where the label will be the form name, and the remaining necessary text data, will be the predictor. As such, I tried a small example just to try it out, using label_txt as the only predictor of the Name, and as expected it scored awfully. But a few things we can gather from this is that, we will definetly need alot more data. 1000 data points isnt going to be enough. Also variance in our case is going to be very important\n",
    "\n",
    "Also should Note a Bag of Words isn't the model that predicts, but simply a way to transform the data into something we can plug in a model.\n",
    "The code below was generated by GPT, although I changed the very few details and explained him our specific case. \n",
    "\n",
    "With that said, besides the Bag Of Words, we need to settle on the initial Model we will use. Taking into account the following details:\n",
    "- We will have a large dataset\n",
    "- For now we are aiming to predict a discrete value\n",
    "- The text might be complex, as well I don't tackle the translation issue in this specific example.\n",
    "\n",
    "I asked our fellow wise bot about what models he thinks would best fit. He recomended Logistic Regression, Random Forest Classifier, other unknown ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2623e020-b520-49a3-a938-f3848a16169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 13.30%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load your CSV into a DataFrame\n",
    "df = pd.read_csv('all_data.csv', on_bad_lines='skip')\n",
    "\n",
    "#print(df)\n",
    "\n",
    "df_cleaned = df.drop(['customname', 'customform', 'fieldparentid', 'blocktype', 'specialtype', 'fieldrelation'], axis=1)\n",
    "\n",
    "df_cleaned['fieldtype'] = df_cleaned['fieldtype'].fillna('string')\n",
    "\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "\n",
    "# Combine the string features into one feature (if needed)\n",
    "# You can choose to combine or process them separately depending on your task\n",
    "df_cleaned['combined_text'] = ' ' + df_cleaned['name'] + ' ' + df_cleaned['fieldparenttype'] + ' ' + df_cleaned['fieldtype']\n",
    "\n",
    "#print(df_cleaned)\n",
    "\n",
    "# Split data into features (X) and the label (y)\n",
    "X = df_cleaned['combined_text']\n",
    "y = df_cleaned['fieldlabel']\n",
    "\n",
    "# Convert text data into Bag of Words\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Initialize the Logistic Regression classifier\n",
    "classifier = LogisticRegression(penalty='l2', C=350.0, solver='liblinear')  # Increase max_iter to ensure convergence\n",
    "\n",
    "# Step 8: Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Step 10: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb352743-1020-4b64-bef7-9571d65957f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
